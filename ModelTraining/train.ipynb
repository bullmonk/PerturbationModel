{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b55c70-acbc-4b9e-8bee-d8b23b1bfea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import argparse\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, validation_curve, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452a0755-0940-457c-ac56-c8cfce35deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# load data\n",
    "\n",
    "# df = dd.read_parquet('../data/test.parquet')\n",
    "df = dd.read_csv('../data/training_data_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24862fb-82f1-407c-97b2-bab4b0e4114f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b263f55-57f3-44ad-ad97-775a4b258dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150207"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.partitions[0].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b5cf76-99ef-4529-8024-0e69ec4aef59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'mlat', 'cos', 'sin', 'lshell', 'ae_index',\n",
       "       'ae_index_lagged_by_5_minutes', 'ae_index_lagged_by_10_minutes',\n",
       "       'ae_index_lagged_by_15_minutes', 'ae_index_lagged_by_20_minutes',\n",
       "       ...\n",
       "       'sym_h_lagged_by_275_minutes', 'sym_h_lagged_by_280_minutes',\n",
       "       'sym_h_lagged_by_285_minutes', 'sym_h_lagged_by_290_minutes',\n",
       "       'sym_h_lagged_by_295_minutes', 'sym_h_lagged_by_300_minutes', 'density',\n",
       "       'density_log10', 'perturbation', 'perturbation_norm'],\n",
       "      dtype='object', length=131)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e6e725-8c9c-4aa4-9f2a-0d9e1459ed6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_ml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m125\u001b[39m]]\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensity_log10\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask_ml'"
     ]
    }
   ],
   "source": [
    "from dask_ml.datasets import train_test_split\n",
    "\n",
    "X = df[df.columns[0:125]]\n",
    "y = df['density_log10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "708528b5-765d-4b02-b5bc-5ab150028bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom IterableDataset to load data in chunks\n",
    "from torch.utils.data import IterableDataset\n",
    "class DaskChunkDataset(IterableDataset):\n",
    "    def __init__(self, X, y, chunk_size=100000):\n",
    "        self.df = dataframe\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        num_chunks = len(self.df) // self.chunk_size\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            # Load one chunk of data at a time\n",
    "            data_chunk = self.dask_df.partitions[chunk_idx].compute().values\n",
    "            for data, label in zip(data_chunk, label_chunk):\n",
    "                yield torch.tensor(data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3c356-755e-4ab5-b7d0-d374b5b77e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e4e53-8c5c-458c-b330-3e25f27a0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # fetch system arguements.\n",
    "    parser = argparse.ArgumentParser(description=\"Train a Neural Network Regressor.\")\n",
    "\n",
    "    parser.add_argument(\"--iData\", type=str, help=\"The file name of input data, .csv or .parquet.\")\n",
    "    parser.add_argument(\"--iIndicies\", type=str, default=\"0:9\", help=\"Numerical Array, referencing the indicies of columns for trainning input.\")\n",
    "    parser.add_argument(\"--target\", type=str, default=\"norm_perturbation\", help=\"Name of the target variable column in the original data.\")\n",
    "    parser.add_argument(\"--disableFeatureStand\", action='store_false', help=\"Disable feature standardization before and after training.\")\n",
    "    parser.add_argument(\"--disableTargetStand\", action='store_false', help=\"Disable target standardization before and after training.\")\n",
    "    parser.add_argument(\"--saveModelPerformance\", action='store_true', help=\"save predicted vs actual training data in a csv file.\")\n",
    "    parser.add_argument(\"--saveFeatureImportances\", action='store_true', help=\"save feature importance in a csv file.\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"dump/netRegressor.joblib\", help=\"file name of the dumped model.\")\n",
    "    parser.add_argument(\"--xscaler\", type=str, default=\"dump/xscaler.joblib\", help=\"file name of the dumped xscaler.\")\n",
    "    parser.add_argument(\"--yscaler\", type=str, default=\"dump/yscaler.joblib\", help=\"file name of the dumped yscaler.\")\n",
    "    parser.add_argument(\"--modelPerf\", type=str, default=\"data/perturbation_cmp_plot_data.csv\", help=\"actual and predicted training target data.\")\n",
    "    parser.add_argument(\"--featureImp\", type=str, default=\"data/density_log10_features_rank.csv\", help=\"feature importance data.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    \n",
    "    # Parse the arguments from the command line\n",
    "    [s, e] = args.iIndicies.split(':')\n",
    "    feature_column_indicies = range(int(s), int(e) + 1)\n",
    "    target_variable_name = args.target\n",
    "    feature_standardization_enabled = args.disableFeatureStand\n",
    "    target_standardization_enabled = args.disableTargetStand\n",
    "    calculate_model_performance = args.saveModelPerformance\n",
    "    calculate_feature_importances = args.saveFeatureImportances\n",
    "\n",
    "    # load data\n",
    "    df = pd.read_csv(args.iData)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df=df.dropna()\n",
    "\n",
    "    # get target variable\n",
    "    target = df[target_variable_name]\n",
    "\n",
    "    # standardization on feature variables    \n",
    "    X = df[df.columns[feature_column_indicies]]\n",
    "    xscaler = None\n",
    "    if feature_standardization_enabled:\n",
    "        xscaler = preprocessing.MinMaxScaler()\n",
    "        names = X.columns\n",
    "        d = xscaler.fit_transform(X)\n",
    "        X = pd.DataFrame(d, columns=names)\n",
    "\n",
    "    y = target\n",
    "    yscaler = None\n",
    "\n",
    "    # conditional standardization on the target varible\n",
    "    if target_standardization_enabled:\n",
    "        yscaler = preprocessing.MinMaxScaler()\n",
    "        d = yscaler.fit_transform(target.values.reshape(-1, 1))\n",
    "        y = pd.DataFrame(d, columns=[target_variable_name])\n",
    "\n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 628)\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    netRegressor = set_device_and_nn(len(feature_column_indicies))\n",
    "\n",
    "    # train model\n",
    "    netRegressor.fit(X_train, y_train)\n",
    "    y_out = netRegressor.predict(X_test)\n",
    "\n",
    "    # dump model\n",
    "    dump(netRegressor, args.model)\n",
    "    if feature_standardization_enabled:\n",
    "        dump(xscaler, args.xscaler)\n",
    "    if target_standardization_enabled:\n",
    "        dump(yscaler, args.yscaler)\n",
    "\n",
    "    r2 = r2_score(y_test, y_out)\n",
    "    print(f'''model r2 score on test data is: {r2}''')\n",
    "\n",
    "    # save data for comparison plot\n",
    "    if calculate_model_performance:\n",
    "        cmp = pd.DataFrame({'actual': y_test.numpy().flatten(), 'predicted': y_out.flatten()})\n",
    "        cmp.to_csv(args.modelPerf, index=False)\n",
    "\n",
    "    # save feature importances data, conditional\n",
    "    if calculate_feature_importances:\n",
    "        r = permutation_importance(netRegressor, X_test, y_test,\n",
    "                            n_repeats=30,\n",
    "                            random_state=0)\n",
    "        feature_importances = r.importances_mean\n",
    "        pd.DataFrame({'features': names, 'importances': feature_importances}).to_csv(args.featureImp, index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "def set_device_and_nn(feature_num):\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using {device} device\")\n",
    "    \n",
    "    # define neural network model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(feature_num, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, 1)\n",
    "    )\n",
    "    \n",
    "    # create skorch wrapper for a regressor.\n",
    "    netRegressor = NeuralNetRegressor(\n",
    "        module=model,\n",
    "        criterion=nn.MSELoss,\n",
    "        optimizer=optim.Adam,\n",
    "        max_epochs=32,\n",
    "        batch_size=128,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return netRegressor\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
